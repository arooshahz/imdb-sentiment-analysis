{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arooshahz/imdb-sentiment-analysis/blob/main/IMDb_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp_PXFzwDXD-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjUtuts0D3XD"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDB dataset consists of 50,000 movie reviews, split evenly into training and test sets. Each sample contains a review text and a binary sentiment label."
      ],
      "metadata": {
        "id": "oHZRT_VByxbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(dataset[\"train\"])\n",
        "test_df = pd.DataFrame(dataset[\"test\"])\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "ELa9utqQxzPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Dq68l6Dbkn"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "zKOaph8Tym0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing values are present in either the text or label columns, indicating a clean dataset suitable for downstream modeling."
      ],
      "metadata": {
        "id": "KhpJnhsWy6wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = train_df[\"label\"].value_counts()\n",
        "\n",
        "label_counts"
      ],
      "metadata": {
        "id": "vU5JpBzWzDek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,4))\n",
        "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
        "plt.xticks([0,1], [\"Negative\", \"Positive\"])\n",
        "plt.title(\"Class Distribution in Training Set\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tTfgNFqnzDtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is perfectly balanced, with an equal number of positive and negative reviews. This allows us to rely on accuracy and F1-score without concerns about class imbalance."
      ],
      "metadata": {
        "id": "krIjSNa1zTBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"word_count\"] = train_df[\"text\"].apply(lambda x: len(x.split()))\n",
        "train_df[\"char_count\"] = train_df[\"text\"].apply(len)\n",
        "\n",
        "train_df[[\"word_count\", \"char_count\"]].describe()"
      ],
      "metadata": {
        "id": "LNRBOCUHzDvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "sns.histplot(train_df[\"word_count\"], bins=50)\n",
        "plt.title(\"Distribution of Review Lengths (Word Count)\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sEsiqzg8zDyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most reviews fall between 100 and 300 words, while a smaller portion of reviews are significantly longer. This observation suggests that truncation will affect only a minority of samples when using Transformer-based models."
      ],
      "metadata": {
        "id": "0qWBU8x0znH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "sns.boxplot(x=\"label\", y=\"word_count\", data=train_df)\n",
        "plt.xticks([0,1], [\"Negative\", \"Positive\"])\n",
        "plt.title(\"Review Length by Sentiment\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Word Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MqzYjJzFzD4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive reviews tend to be slightly longer on average, which may indicate that users elaborate more when expressing positive opinions."
      ],
      "metadata": {
        "id": "5Hex40Eaz13Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.sort_values(\"word_count\").head(3)[[\"text\", \"label\", \"word_count\"]]"
      ],
      "metadata": {
        "id": "jXsnZOkYzD6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.sort_values(\"word_count\", ascending=False).head(3)[[\"label\", \"word_count\"]]"
      ],
      "metadata": {
        "id": "3ep9xEvrz3M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_html(text):\n",
        "    return bool(re.search(r\"<.*?>\", text))\n",
        "\n",
        "train_df[\"has_html\"] = train_df[\"text\"].apply(contains_html)\n",
        "train_df[\"has_html\"].mean()"
      ],
      "metadata": {
        "id": "iIbVOaX6z3PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small fraction of reviews contain HTML tags, which should be considered during preprocessing for baseline models."
      ],
      "metadata": {
        "id": "FzGt13du0bq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_reviews = train_df[train_df[\"label\"] == 1][\"text\"]\n",
        "negative_reviews = train_df[train_df[\"label\"] == 0][\"text\"]"
      ],
      "metadata": {
        "id": "FCdakNKGz3Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    # remove HTML\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    # keep letters only\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "CbUPtTes0kYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_clean = positive_reviews.apply(clean_text)\n",
        "negative_clean = negative_reviews.apply(clean_text)"
      ],
      "metadata": {
        "id": "VB4i_BDL0kan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = Counter(\" \".join(positive_clean).split())\n",
        "negative_words = Counter(\" \".join(negative_clean).split())"
      ],
      "metadata": {
        "id": "_SiYSgi30kc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "for stopword in ENGLISH_STOP_WORDS:\n",
        "    positive_words.pop(stopword, None)\n",
        "    negative_words.pop(stopword, None)"
      ],
      "metadata": {
        "id": "WBAnX9QI0kfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_pos = positive_words.most_common(20)\n",
        "top_neg = negative_words.most_common(20)\n",
        "\n",
        "top_pos, top_neg"
      ],
      "metadata": {
        "id": "WIre5i9v0khy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_words(word_counts, title):\n",
        "    words, counts = zip(*word_counts)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.barplot(x=list(counts), y=list(words))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Frequency\")\n",
        "    plt.ylabel(\"Word\")\n",
        "    plt.show()\n",
        "\n",
        "plot_top_words(top_pos, \"Top 20 Words in Positive Reviews\")\n",
        "plot_top_words(top_neg, \"Top 20 Words in Negative Reviews\")"
      ],
      "metadata": {
        "id": "ecpazbXdz3UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive reviews frequently include words such as great, like, and love, while negative reviews are dominated by terms like bad, dont, and like. This clear lexical separation indicates that sentiment is strongly reflected in word choice, making the dataset suitable for both classical and Transformer-based text classification models."
      ],
      "metadata": {
        "id": "k2rCkag031w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relative_freq(word, counter, total_words):\n",
        "    return counter[word] / total_words\n",
        "\n",
        "total_pos_words = sum(positive_words.values())\n",
        "total_neg_words = sum(negative_words.values())\n",
        "\n",
        "diff_words = []\n",
        "\n",
        "for word in set(list(positive_words.keys()) + list(negative_words.keys())):\n",
        "    pos_freq = get_relative_freq(word, positive_words, total_pos_words)\n",
        "    neg_freq = get_relative_freq(word, negative_words, total_neg_words)\n",
        "    diff_words.append((word, pos_freq - neg_freq))\n",
        "\n",
        "diff_words_sorted = sorted(diff_words, key=lambda x: abs(x[1]), reverse=True)\n",
        "diff_words_sorted[:20]\n"
      ],
      "metadata": {
        "id": "onG2RW-d3wWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain words exhibit a strong sentiment polarity, appearing disproportionately in either positive or negative reviews. This observation further supports the effectiveness of lexical features for sentiment classification."
      ],
      "metadata": {
        "id": "-I6O6oSO4iyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "I36yL4Jl3wYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df[\"text\"]\n",
        "y = train_df[\"label\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "MuyHHZZ9SAd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stratified split is used to preserve the class distribution in both training and validation sets."
      ],
      "metadata": {
        "id": "SFSlhfW3SR38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        max_features=20000,\n",
        "        ngram_range=(1,2),\n",
        "        stop_words=\"english\"\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000))\n",
        "])"
      ],
      "metadata": {
        "id": "fEJwnFE2SAf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "biCSVxo1SAjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = logreg_pipeline.predict(X_val)\n",
        "\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "acc, f1"
      ],
      "metadata": {
        "id": "3jPG4PGgSAlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_val, y_pred, target_names=[\"Negative\", \"Positive\"]))"
      ],
      "metadata": {
        "id": "GrmJVtgdSAoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression baseline achieves strong performance, indicating that sentiment in the IMDB dataset is highly correlated with lexical features."
      ],
      "metadata": {
        "id": "L08jKdpBSknJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        max_features=20000,\n",
        "        ngram_range=(1,2),\n",
        "        stop_words=\"english\"\n",
        "    )),\n",
        "    (\"clf\", LinearSVC())\n",
        "])"
      ],
      "metadata": {
        "id": "-bm0BEExSAs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RG13ismVSAvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_svm = svm_pipeline.predict(X_val)\n",
        "\n",
        "acc_svm = accuracy_score(y_val, y_pred_svm)\n",
        "f1_svm = f1_score(y_val, y_pred_svm)\n",
        "\n",
        "acc_svm, f1_svm"
      ],
      "metadata": {
        "id": "f4_aMkesSAx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame({\n",
        "    \"Model\": [\"Logistic Regression\", \"Linear SVM\"],\n",
        "    \"Accuracy\": [acc, acc_svm],\n",
        "    \"F1-score\": [f1, f1_svm]\n",
        "})\n",
        "\n",
        "results_df"
      ],
      "metadata": {
        "id": "NNt-m7orTD0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While both classical models perform strongly, they rely heavily on surface-level lexical features and fail to capture deeper contextual relationships, motivating the use of Transformer-based models."
      ],
      "metadata": {
        "id": "jDXacyvCTJxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = logreg_pipeline.named_steps[\"tfidf\"].get_feature_names_out()\n",
        "coefficients = logreg_pipeline.named_steps[\"clf\"].coef_[0]\n",
        "\n",
        "top_positive = sorted(\n",
        "    zip(feature_names, coefficients),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")[:20]\n",
        "\n",
        "top_negative = sorted(\n",
        "    zip(feature_names, coefficients),\n",
        "    key=lambda x: x[1]\n",
        ")[:20]\n",
        "\n",
        "top_positive, top_negative"
      ],
      "metadata": {
        "id": "nxqex89sTD2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most influential features align with intuitive sentiment-bearing words, confirming that the model learns meaningful patterns from the data."
      ],
      "metadata": {
        "id": "Yehmd07LTO6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "qndf3FTvUP8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained using GPU acceleration to significantly reduce training time."
      ],
      "metadata": {
        "id": "AF_Mk5SFVE2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets evaluate accelerate"
      ],
      "metadata": {
        "id": "njeXCj9XUP_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3FuzdUXrUQB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "Ks4_qxMqUQGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We directly use the HuggingFace Dataset object to ensure seamless integration with the Trainer API."
      ],
      "metadata": {
        "id": "dwqtf-dhVjHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "oEXq9AF0UQIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bert-base-uncased is chosen as a strong and widely used baseline Transformer model for English text classification."
      ],
      "metadata": {
        "id": "6bkAqECRVwY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 256"
      ],
      "metadata": {
        "id": "3Tz0t427UQLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length\n",
        "    )"
      ],
      "metadata": {
        "id": "V2E-e7R6UQOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "qB-kAe0oTD8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "9_GcSU4pSA0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"test\"]"
      ],
      "metadata": {
        "id": "XzKJfv11V3vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(\n",
        "        predictions=predictions, references=labels\n",
        "    )\n",
        "    f1 = f1_metric.compute(\n",
        "        predictions=predictions, references=labels\n",
        "    )\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"f1\": f1[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "eFV1rcxdV3yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both accuracy and F1-score are reported to ensure a comprehensive evaluation of model performance."
      ],
      "metadata": {
        "id": "XFVht7gcWDRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "um42CRpOV306"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-imdb\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100\n",
        ")"
      ],
      "metadata": {
        "id": "ZPA5wmCxV33T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "Uuo-7bmuV35t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "VnZTVKqhXco1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "NctQ-yP6Xcrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  'eval_loss': ...,\n",
        "  'eval_accuracy': 0.92,\n",
        "  'eval_f1': 0.92\n",
        "}"
      ],
      "metadata": {
        "id": "q5N5qNpkXcvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuned BERT significantly outperforms classical baselines by capturing contextual and semantic information beyond surface-level lexical features."
      ],
      "metadata": {
        "id": "opO5vjVqXlxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "predictions = trainer.predict(eval_dataset)\n",
        "\n",
        "logits = predictions.predictions\n",
        "y_true = predictions.label_ids\n",
        "y_pred = np.argmax(logits, axis=1)"
      ],
      "metadata": {
        "id": "QLDJhGyQXcx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = []\n",
        "\n",
        "for i in range(len(y_true)):\n",
        "    if y_true[i] != y_pred[i]:\n",
        "        errors.append({\n",
        "            \"text\": dataset[\"test\"][i][\"text\"],\n",
        "            \"true_label\": y_true[i],\n",
        "            \"pred_label\": y_pred[i]\n",
        "        })\n",
        "\n",
        "error_df = pd.DataFrame(errors)\n",
        "error_df.head()"
      ],
      "metadata": {
        "id": "Jvb2nvcfluYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_df.sample(5)"
      ],
      "metadata": {
        "id": "WDgeywmIkrBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some misclassified samples contain mixed sentiments or sarcasm, which remains challenging even for Transformer-based models."
      ],
      "metadata": {
        "id": "vwb3_7LXqMub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error_df[\"word_count\"] = error_df[\"text\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "error_df[\"word_count\"].describe()"
      ],
      "metadata": {
        "id": "AO4vnt1RkrEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"word_count\"].describe()"
      ],
      "metadata": {
        "id": "MPcCssuOkrOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Misclassified reviews tend to be longer on average, suggesting that truncation may contribute to information loss."
      ],
      "metadata": {
        "id": "qKSSkEDkqRqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error_df[error_df[\"text\"].str.contains(\"not\", case=False)].head(3)"
      ],
      "metadata": {
        "id": "MgigqH4eXdCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negation handling and subtle sentiment shifts remain a common source of error, even for pretrained language models."
      ],
      "metadata": {
        "id": "GNxfPHfSqV-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Negative\", \"Positive\"],\n",
        "            yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix â€” BERT on IMDB\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "71baKX5sXw9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix shows balanced performance across both classes, with no strong bias toward either sentiment."
      ],
      "metadata": {
        "id": "ib6fjSztqZtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_results = pd.DataFrame({\n",
        "    \"Model\": [\n",
        "        \"Logistic Regression (TF-IDF)\",\n",
        "        \"Linear SVM (TF-IDF)\",\n",
        "        \"BERT (Fine-tuned)\"\n",
        "    ],\n",
        "    \"Accuracy\": [\n",
        "        acc,\n",
        "        acc_svm,\n",
        "        trainer.evaluate()[\"eval_accuracy\"]\n",
        "    ],\n",
        "    \"F1-score\": [\n",
        "        f1,\n",
        "        f1_svm,\n",
        "        trainer.evaluate()[\"eval_f1\"]\n",
        "    ]\n",
        "})\n",
        "\n",
        "final_results"
      ],
      "metadata": {
        "id": "QVtc0tM7XxFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_results.set_index(\"Model\")[[\"Accuracy\", \"F1-score\"]].plot.bar(\n",
        "    figsize=(8,4), ylim=(0.8,1.0)\n",
        ")\n",
        "plt.title(\"Model Comparison on IMDB Dataset\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5o5lD9hPXxLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuned BERT achieves the best overall performance, outperforming classical baselines by leveraging contextual semantic representations."
      ],
      "metadata": {
        "id": "65yxR19Fqg-V"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPskyRu01Fpbj30XCikxD8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}